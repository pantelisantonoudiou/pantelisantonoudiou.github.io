---
title: "Mechanistic Insights into How Shallow CNNs Classify EEG Seizure Activity"
excerpt: "Investigating how shallow convolutional neural networks internally represent and classify EEG segments in a seizure detection task."
layout: single
author_profile: true
read_time: true
comments: true
share: true
categories:
  - Ongoing Projects
tags:
  - Deep Leaning
  - EEG
  - Mechanistic Interpretability
---

Deep learning is increasingly used to support automated event detection, with CNNs achieving strong performance on EEG classification tasks. However, their black-box nature limits their acceptance in neuroscience and clinical settings, where transparency and interpretability remain critical, and manual curation continues to serve as the gold standard.

In this project, we are investigating the internal representations of a shallow CNN trained to detect electrographic seizures. We are specifically interested in understanding how the model makes decisions at the single neuron and/or ensemble level and going beyond standard input attribution methods.

<a href="/assets/images/cnn_interp.png">
  <img src="/assets/images/cnn_interp.png" alt="CNN Interpretability Pipeline" style="max-width: 600px; margin-bottom: 1rem; border-radius: 6px;">
</a>
<p style="font-size: 0.9rem; color: #666;">
  <em>Figure: A) Model architecture, B) Input types, C) Model schematic, D) Separation of activations using PCA(nPCs=100)-UMAP, E) Example ensemble identification strategy, F) Silencing of seizure ensembles reduces model prediction to chance only for seizure traces.</em>
</p>

**Some early findings include:**

 1) Separation of seizure and non-seizure traces begins from the first convolutional layer.

 2) Intermediate layers form sparse representations with high-redundancy across layers.

 3) Silencing of the class-specific ensembles predictably alters model decision.

### Coming soon
Stay tuned for the arXiv paper where we will be exploring human interpretable features in inputs, dense neurons, and more.
---
